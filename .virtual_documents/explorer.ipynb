#!gcloud auth login


#!gsutil -m cp -r gs://the_impostor_hunt/data ./  


# ler o arquivo data/train.csv
# para cada id na tabela train_roles no formato com 4 numeros (0001, 0002, 0010)
# ler cada um dos arquivos na pasta data/train/pasta


import pandas as pd
import json
import re
import string
import nltk
import math
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support
from scipy.sparse import hstack, csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack




nltk.download('stopwords')
nltk.download('punkt_tab')


train_rules = pd.read_csv("data/train.csv")


train_rules.head()


train_rules['fake_text_id'] = train_rules['real_text_id'].apply(lambda x: 2 if x == 1 else 1)


train_rules.head()


train_rules['article'] = train_rules['id'].apply(lambda x: str(x).zfill(4))


train_rules.head()


def cria_caminho_article(text_id, article):
    caminho = f"data/train/article_{article}/file_{text_id}.txt"
    return caminho


train_rules['real_text_file'] = train_rules[['real_text_id', 'article']].apply(lambda x: cria_caminho_article(x['real_text_id'], x['article']), axis=1)
train_rules['fake_text_file'] = train_rules[['fake_text_id', 'article']].apply(lambda x: cria_caminho_article(x['fake_text_id'], x['article']), axis=1)


train_rules.head()


def leitura_arquivo(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read().strip()
    return text


train_rules['real_text'] = train_rules['real_text_file'].apply(leitura_arquivo)


train_rules['fake_text'] = train_rules['fake_text_file'].apply(leitura_arquivo)


train_rules


# Criar os dois dataframes com as colunas relevantes, incluindo o artigo
df_real = train_rules[['article', 'real_text']].copy()
df_real.columns = ['article', 'texto']  # Padroniza nomes
df_real['label'] = 1

df_fake = train_rules[['article', 'fake_text']].copy()
df_fake.columns = ['article', 'texto']
df_fake['label'] = 0

# Concatenar os dois dataframes (real + fake)
df_completo = pd.concat([df_real, df_fake], ignore_index=True)

# Exibir resultado
print(df_completo.head())


df_completo


n_artigo = '0090'
print(df_completo[df_completo['article']==n_artigo].label.iloc[0])
df_completo[df_completo['article']==n_artigo].texto.iloc[0]


print(df_completo[df_completo['article']==n_artigo].label.iloc[1])
df_completo[df_completo['article']==n_artigo].texto.iloc[1]


df_completo['label'].value_counts().plot(kind='bar', title='Distribuição das classes (real vs. fake)')



df_completo['texto_len'] = df_completo['texto'].apply(lambda x: len(x.split()))
df_completo.groupby('label')['texto_len'].describe()


from wordcloud import WordCloud
import matplotlib.pyplot as plt

real_text = ' '.join(df_completo[df_completo['label'] == 1]['texto'])
fake_text = ' '.join(df_completo[df_completo['label'] == 0]['texto'])

plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.imshow(WordCloud(width=800, height=400).generate(real_text))
plt.title('Real Texts')
plt.axis('off')

plt.subplot(1,2,2)
plt.imshow(WordCloud(width=800, height=400).generate(fake_text))
plt.title('Fake Texts')
plt.axis('off')
plt.show()




artigos = df_completo['article'].unique()
sim_scores = []

vectorizer = TfidfVectorizer()

for a in artigos:
    pares = df_completo[df_completo['article'] == a]
    if len(pares) == 2:
        vectors = vectorizer.fit_transform(pares['texto'])
        score = cosine_similarity(vectors[0], vectors[1])[0][0]
        sim_scores.append(score)

print(f'Média de similaridade: {sum(sim_scores)/len(sim_scores):.2f}')


#real_text


#fake_text


# obtendo caracteres não latinos
def count_non_latin(text):
    return len(re.findall(r'[^\x00-\x7F]', text))  # ASCII padrão

df_completo['non_latin_chars'] = df_completo['texto'].apply(count_non_latin)


df_completo.sort_values(by='non_latin_chars', ascending=False)[:10]


df_completo.groupby('label')[['texto_len', 'non_latin_chars']].describe()


# números de pontuações


df_completo['num_pontuacao'] = df_completo['texto'].apply(lambda x: sum(1 for c in x if c in string.punctuation))


df_completo


df_completo.groupby('label')['num_pontuacao'].describe()


df_completo['num_linhas'] = df_completo['texto'].apply(lambda x: x.count('\n'))


df_completo.groupby('label')['num_linhas'].describe()


df_completo


# proporção de stopword
stopwords_set = set(stopwords.words('english'))

def stopword_ratio(text):
    words = word_tokenize(text.lower())
    if len(words) == 0:
        return 0
    stop_count = sum(1 for w in words if w in stopwords_set)
    return stop_count / len(words)

df_completo['stopword_ratio'] = df_completo['texto'].apply(stopword_ratio)





# tamanho das words
# se for maior que 15 ele soma 1
df_completo['long_words'] = df_completo['texto'].apply(lambda x: sum(1 for w in x.split() if len(w) > 15))


df_completo.head()


df_completo.sort_values(by='long_words', ascending=False)[:10]


def entropy(text):
    if len(text) == 0:
        return 0
    probs = [v / len(text) for v in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs if p > 0)

# Aplicar no dataframe
df_completo['entropia'] = df_completo['texto'].apply(entropy)


df_completo


df_completo.groupby('label')['entropia'].describe()


# Selecionar apenas as colunas numéricas que criamos
features_numericas = df_completo[['texto_len', 'non_latin_chars', 'num_pontuacao', 
                                  'num_linhas', 'stopword_ratio', 'long_words', 'entropia']]

# Calcular a matriz de correlação
correlacao = features_numericas.corr()

# Plotar o heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(correlacao, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title("Correlação entre features numéricas")
plt.show()


df_completo.head()





# Selecionar apenas as features úteis e o rótulo
df_final = df_completo[['article', 'texto', 'texto_len', 'stopword_ratio', 'entropia', 'label']].copy()

df_final.head()


df_final.head()


X = df_final[['texto_len', 'stopword_ratio', 'entropia']]
y = df_final['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))


# Vetorização TF-IDF
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normalização das features manuais
from sklearn.preprocessing import StandardScaler
X_extra = StandardScaler().fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combinação
X_combined = hstack([X_tfidf, X_extra])

# Modelo (idêntico ao anterior)
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, stratify=y, random_state=42)
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))


# TF-IDF com n-grams, stopwords e mais features
tfidf = TfidfVectorizer(
    max_features=1000,
    stop_words='english',
    sublinear_tf=True
)
#tfidf = TfidfVectorizer(max_features=500)
#tfidf = TfidfVectorizer(max_features=500)

X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normalização das features manuais
scaler = StandardScaler()
X_extra = scaler.fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combinação
X_combined = hstack([X_tfidf, X_extra])

# Rótulo
y = df_final['label']

# Split com estratificação
X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y, test_size=0.2, stratify=y, random_state=42
)

# Modelo
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Avaliação
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))


X_test











# Vetorização TF-IDF
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normalização das features manuais
from sklearn.preprocessing import StandardScaler
X_extra = StandardScaler().fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combinação
X_combined = hstack([X_tfidf, X_extra])

# Modelo (idêntico ao anterior)
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, stratify=y, random_state=42)
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))



# Vetorização TF-IDF
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normalização das features manuais
X_extra = StandardScaler().fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combinação e conversão para CSR (permite indexação)
X_combined = hstack([X_tfidf, X_extra])
X_combined = csr_matrix(X_combined)

# Rótulo
y = df_final['label'].values

# K-Fold Stratificado
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Armazena métricas
accuracies = []
precisions = []
recalls = []
f1s = []

for train_idx, test_idx in kf.split(X_combined, y):
    X_train, X_test = X_combined[train_idx], X_combined[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')

    accuracies.append(acc)
    precisions.append(precision)
    recalls.append(recall)
    f1s.append(f1)

# Resultados médios
print("📊 Resultados médios em 5 folds:")
print(f"Accuracy:  {np.mean(accuracies):.2f}")
print(f"Precision: {np.mean(precisions):.2f}")
print(f"Recall:    {np.mean(recalls):.2f}")
print(f"F1-score:  {np.mean(f1s):.2f}")



import torch
print(torch.cuda.is_available())



