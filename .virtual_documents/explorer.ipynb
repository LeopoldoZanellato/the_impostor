#!gcloud auth login


#!gsutil -m cp -r gs://the_impostor_hunt/data ./  


# ler o arquivo data/train.csv
# para cada id na tabela train_roles no formato com 4 numeros (0001, 0002, 0010)
# ler cada um dos arquivos na pasta data/train/pasta


import pandas as pd
import json
import re
import string
import nltk
import math
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support
from scipy.sparse import hstack, csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack




nltk.download('stopwords')
nltk.download('punkt_tab')


train_rules = pd.read_csv("data/train.csv")


train_rules.head()


train_rules['fake_text_id'] = train_rules['real_text_id'].apply(lambda x: 2 if x == 1 else 1)


train_rules.head()


train_rules['article'] = train_rules['id'].apply(lambda x: str(x).zfill(4))


train_rules.head()


def cria_caminho_article(text_id, article):
    caminho = f"data/train/article_{article}/file_{text_id}.txt"
    return caminho


train_rules['real_text_file'] = train_rules[['real_text_id', 'article']].apply(lambda x: cria_caminho_article(x['real_text_id'], x['article']), axis=1)
train_rules['fake_text_file'] = train_rules[['fake_text_id', 'article']].apply(lambda x: cria_caminho_article(x['fake_text_id'], x['article']), axis=1)


train_rules.head()


def leitura_arquivo(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        text = f.read().strip()
    return text


train_rules['real_text'] = train_rules['real_text_file'].apply(leitura_arquivo)


train_rules['fake_text'] = train_rules['fake_text_file'].apply(leitura_arquivo)


train_rules


# Criar os dois dataframes com as colunas relevantes, incluindo o artigo
df_real = train_rules[['article', 'real_text']].copy()
df_real.columns = ['article', 'texto']  # Padroniza nomes
df_real['label'] = 1

df_fake = train_rules[['article', 'fake_text']].copy()
df_fake.columns = ['article', 'texto']
df_fake['label'] = 0

# Concatenar os dois dataframes (real + fake)
df_completo = pd.concat([df_real, df_fake], ignore_index=True)

# Exibir resultado
print(df_completo.head())


df_completo


n_artigo = '0090'
print(df_completo[df_completo['article']==n_artigo].label.iloc[0])
df_completo[df_completo['article']==n_artigo].texto.iloc[0]


print(df_completo[df_completo['article']==n_artigo].label.iloc[1])
df_completo[df_completo['article']==n_artigo].texto.iloc[1]


df_completo['label'].value_counts().plot(kind='bar', title='Distribui√ß√£o das classes (real vs. fake)')



df_completo['texto_len'] = df_completo['texto'].apply(lambda x: len(x.split()))
df_completo.groupby('label')['texto_len'].describe()


from wordcloud import WordCloud
import matplotlib.pyplot as plt

real_text = ' '.join(df_completo[df_completo['label'] == 1]['texto'])
fake_text = ' '.join(df_completo[df_completo['label'] == 0]['texto'])

plt.figure(figsize=(10,5))
plt.subplot(1,2,1)
plt.imshow(WordCloud(width=800, height=400).generate(real_text))
plt.title('Real Texts')
plt.axis('off')

plt.subplot(1,2,2)
plt.imshow(WordCloud(width=800, height=400).generate(fake_text))
plt.title('Fake Texts')
plt.axis('off')
plt.show()




artigos = df_completo['article'].unique()
sim_scores = []

vectorizer = TfidfVectorizer()

for a in artigos:
    pares = df_completo[df_completo['article'] == a]
    if len(pares) == 2:
        vectors = vectorizer.fit_transform(pares['texto'])
        score = cosine_similarity(vectors[0], vectors[1])[0][0]
        sim_scores.append(score)

print(f'M√©dia de similaridade: {sum(sim_scores)/len(sim_scores):.2f}')


#real_text


#fake_text


# obtendo caracteres n√£o latinos
def count_non_latin(text):
    return len(re.findall(r'[^\x00-\x7F]', text))  # ASCII padr√£o

df_completo['non_latin_chars'] = df_completo['texto'].apply(count_non_latin)


df_completo.sort_values(by='non_latin_chars', ascending=False)[:10]


df_completo.groupby('label')[['texto_len', 'non_latin_chars']].describe()


# n√∫meros de pontua√ß√µes


df_completo['num_pontuacao'] = df_completo['texto'].apply(lambda x: sum(1 for c in x if c in string.punctuation))


df_completo


df_completo.groupby('label')['num_pontuacao'].describe()


df_completo['num_linhas'] = df_completo['texto'].apply(lambda x: x.count('\n'))


df_completo.groupby('label')['num_linhas'].describe()


df_completo


# propor√ß√£o de stopword
stopwords_set = set(stopwords.words('english'))

def stopword_ratio(text):
    words = word_tokenize(text.lower())
    if len(words) == 0:
        return 0
    stop_count = sum(1 for w in words if w in stopwords_set)
    return stop_count / len(words)

df_completo['stopword_ratio'] = df_completo['texto'].apply(stopword_ratio)





# tamanho das words
# se for maior que 15 ele soma 1
df_completo['long_words'] = df_completo['texto'].apply(lambda x: sum(1 for w in x.split() if len(w) > 15))


df_completo.head()


df_completo.sort_values(by='long_words', ascending=False)[:10]


def entropy(text):
    if len(text) == 0:
        return 0
    probs = [v / len(text) for v in Counter(text).values()]
    return -sum(p * math.log2(p) for p in probs if p > 0)

# Aplicar no dataframe
df_completo['entropia'] = df_completo['texto'].apply(entropy)


df_completo


df_completo.groupby('label')['entropia'].describe()


# Selecionar apenas as colunas num√©ricas que criamos
features_numericas = df_completo[['texto_len', 'non_latin_chars', 'num_pontuacao', 
                                  'num_linhas', 'stopword_ratio', 'long_words', 'entropia']]

# Calcular a matriz de correla√ß√£o
correlacao = features_numericas.corr()

# Plotar o heatmap
plt.figure(figsize=(10, 7))
sns.heatmap(correlacao, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title("Correla√ß√£o entre features num√©ricas")
plt.show()


df_completo.head()





# Selecionar apenas as features √∫teis e o r√≥tulo
df_final = df_completo[['article', 'texto', 'texto_len', 'stopword_ratio', 'entropia', 'label']].copy()

df_final.head()


df_final.head()


X = df_final[['texto_len', 'stopword_ratio', 'entropia']]
y = df_final['label']

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))


# Vetoriza√ß√£o TF-IDF
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normaliza√ß√£o das features manuais
from sklearn.preprocessing import StandardScaler
X_extra = StandardScaler().fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combina√ß√£o
X_combined = hstack([X_tfidf, X_extra])

# Modelo (id√™ntico ao anterior)
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, stratify=y, random_state=42)
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))


# TF-IDF com n-grams, stopwords e mais features
tfidf = TfidfVectorizer(
    max_features=1000,
    stop_words='english',
    sublinear_tf=True
)
#tfidf = TfidfVectorizer(max_features=500)
#tfidf = TfidfVectorizer(max_features=500)

X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normaliza√ß√£o das features manuais
scaler = StandardScaler()
X_extra = scaler.fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combina√ß√£o
X_combined = hstack([X_tfidf, X_extra])

# R√≥tulo
y = df_final['label']

# Split com estratifica√ß√£o
X_train, X_test, y_train, y_test = train_test_split(
    X_combined, y, test_size=0.2, stratify=y, random_state=42
)

# Modelo
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Avalia√ß√£o
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))


X_test











# Vetoriza√ß√£o TF-IDF
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normaliza√ß√£o das features manuais
from sklearn.preprocessing import StandardScaler
X_extra = StandardScaler().fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combina√ß√£o
X_combined = hstack([X_tfidf, X_extra])

# Modelo (id√™ntico ao anterior)
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, stratify=y, random_state=42)
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))



# Vetoriza√ß√£o TF-IDF
tfidf = TfidfVectorizer(max_features=500)
X_tfidf = tfidf.fit_transform(df_final['texto'])

# Normaliza√ß√£o das features manuais
X_extra = StandardScaler().fit_transform(df_final[['texto_len', 'stopword_ratio', 'entropia']])

# Combina√ß√£o e convers√£o para CSR (permite indexa√ß√£o)
X_combined = hstack([X_tfidf, X_extra])
X_combined = csr_matrix(X_combined)

# R√≥tulo
y = df_final['label'].values

# K-Fold Stratificado
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Armazena m√©tricas
accuracies = []
precisions = []
recalls = []
f1s = []

for train_idx, test_idx in kf.split(X_combined, y):
    X_train, X_test = X_combined[train_idx], X_combined[test_idx]
    y_train, y_test = y[train_idx], y[test_idx]

    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')

    accuracies.append(acc)
    precisions.append(precision)
    recalls.append(recall)
    f1s.append(f1)

# Resultados m√©dios
print("üìä Resultados m√©dios em 5 folds:")
print(f"Accuracy:  {np.mean(accuracies):.2f}")
print(f"Precision: {np.mean(precisions):.2f}")
print(f"Recall:    {np.mean(recalls):.2f}")
print(f"F1-score:  {np.mean(f1s):.2f}")



import torch
print(torch.cuda.is_available())



